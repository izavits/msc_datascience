{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4910505b",
   "metadata": {},
   "source": [
    "# Pre-trained Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bee4b5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 3000000\n"
     ]
    }
   ],
   "source": [
    "# Play with pre-trained word embeddings\n",
    "# Need to download GoogleNews-vectors-negative300.bin or other word2vec embeddings and copy to ./data folder\n",
    "\n",
    "\n",
    "# We will use the gensim library to import the word vectors\n",
    "import gensim\n",
    "\n",
    "# Load the word embeddings\n",
    "# (this is just a simple structure. Each word is a vector)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Get the vocabulary (i.e. the unique words that were used to train these embeddings)\n",
    "vocab = model.vocab.keys()\n",
    "\n",
    "# Get the size of the Vocabulary\n",
    "wordsInVocab = len(vocab)\n",
    "print(f'Vocab length: {wordsInVocab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92d9c5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
      " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
      "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
      "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
      " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
      " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
      "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
      "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
      " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
      " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
      " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
      "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
      " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
      "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
      "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
      "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
      " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
      " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
      "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
      "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
      "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
      "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
      " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
      " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
      "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
      "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
      " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
      "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
      " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
      " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
      " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
      " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
      "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
      " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
      "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
      "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
      " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
      " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
      " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
      " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
      " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
      " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
      " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
      " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
      "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
      " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
      "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
      "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
      " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
      "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
      " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
      "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
      " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
      " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
      " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
      " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
      " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
      " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
      " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
      "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
      "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
      " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
      "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
      " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
      "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
      " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
      "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
      " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
      " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
      " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
      "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
      "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
      "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
      "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
      "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n",
      "Dimensions: 300\n"
     ]
    }
   ],
   "source": [
    "# Let's see the vector of a random word\n",
    "\n",
    "print(model['computer'])\n",
    "print(f'Dimensions: {len(model[\"computer\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785458c2",
   "metadata": {},
   "source": [
    "## Compute similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1c2e800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between (dog, cat): 0.760945737361908\n",
      "Similarity between (king, queen): 0.6510956883430481\n",
      "Similarity between (car, computer): 0.246127188205719\n"
     ]
    }
   ],
   "source": [
    "# Compute similarities between words\n",
    "\n",
    "print(f'Similarity between (dog, cat): {model.similarity(\"dog\", \"cat\")}')\n",
    "print(f'Similarity between (king, queen): {model.similarity(\"king\", \"queen\")}')\n",
    "print(f'Similarity between (car, computer): {model.similarity(\"car\", \"computer\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c3fee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spain', 0.6375303268432617),\n",
       " ('french', 0.6326056718826294),\n",
       " ('germany', 0.6314354538917542),\n",
       " ('europe', 0.6264256238937378),\n",
       " ('italy', 0.6257959008216858)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look up most similar words\n",
    "model.most_similar('france', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc956cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: this is sentence\n",
      "Sentence 2: this is also sentence\n",
      "Similarity = 0.960\n"
     ]
    }
   ],
   "source": [
    "# Compute similarities between sentences\n",
    "\n",
    "s1 = \"This is a sentence\"\n",
    "s2 = \"This is also a sentence\"\n",
    "\n",
    "# Normalize sentences: remove words not in vocabulary\n",
    "tokens = s1.split()\n",
    "s1_final = ''\n",
    "for t in tokens:\n",
    "    if t.lower() in model.vocab:\n",
    "        s1_final += t.lower() + ' '\n",
    "s1_final = s1_final.strip()\n",
    "\n",
    "tokens = s2.split()\n",
    "s2_final = ''\n",
    "for t in tokens:\n",
    "    if t.lower() in model.vocab:\n",
    "        s2_final += t.lower() + ' '\n",
    "s2_final = s2_final.strip()\n",
    "\n",
    "print(f'Sentence 1: {s1_final}')\n",
    "print(f'Sentence 2: {s2_final}')\n",
    "sml = model.n_similarity(s1_final.split(), s2_final.split())\n",
    "print('Similarity = %.3f' % sml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43eda2a",
   "metadata": {},
   "source": [
    "## Vector Arithmetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f95d246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071)]\n"
     ]
    }
   ],
   "source": [
    "# Most famous example:\n",
    "\n",
    "# \"Man to Woman is King to X\", what is X? (Answer: Queen)\n",
    "\n",
    "# We can apply basic arithmetic to Word2Vec vectors:\n",
    "# King - Man + Woman = ?\n",
    "\n",
    "# This means that if we take the notion of King and subtract the notion of Man \n",
    "# and add the notion of Woman, we get the notion of Queen\n",
    "\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e81e5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Japan', 0.8167769908905029)]\n",
      "\n",
      "[('princess', 0.7421581745147705)]\n"
     ]
    }
   ],
   "source": [
    "# Similar examples\n",
    "result = model.most_similar(positive=['Tokyo', 'France'], negative=['Paris'], topn=1)\n",
    "print(result)\n",
    "\n",
    "print('')\n",
    "\n",
    "result = model.most_similar(positive=['girl', 'prince'], negative=['boy'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22eb34a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nurse', 0.6588720679283142)]\n",
      "\n",
      "[('homemaker', 0.4312755763530731)]\n",
      "\n",
      "[('manicurist', 0.5636395215988159)]\n"
     ]
    }
   ],
   "source": [
    "# Probably not the best results: \n",
    "# but the model learns what we feed it\n",
    "\n",
    "result = model.most_similar(positive=['she', 'doctor'], negative=['he'], topn=1)\n",
    "print(result)\n",
    "\n",
    "print('')\n",
    "\n",
    "result = model.most_similar(positive=['woman', 'computer_programer'], negative=['man'], topn=1)\n",
    "print(result)\n",
    "\n",
    "print('')\n",
    "\n",
    "result = model.most_similar(positive=['she', 'janitor'], negative=['he'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c05188",
   "metadata": {},
   "source": [
    "*Gender bias and other types of bias is something we must deal with if we want to move toward ethical and transparent AI solutions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df4930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
