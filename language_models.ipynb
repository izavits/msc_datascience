{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab00f0a8",
   "metadata": {},
   "source": [
    "# Examples with probabilistic language models (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611fbb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with some basics:\n",
    "# Given a sentence, let's see how to perform some counts\n",
    "# with the help of Counters and python dicts\n",
    "\n",
    "from nltk import trigrams\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3dc903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sentence', 'that', 'we', 'want', 'to', 'parse', 'and', 'this', 'is', 'done', 'with', 'nltk', 'and', 'python', 'collections']\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "sentence = 'this is a sentence that we want to parse and this is done with nltk and python collections'\n",
    "\n",
    "# Simple tokenization (separation of tokens via space character)\n",
    "sentence = sentence.split()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9a88cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None this\n",
      "None this is\n",
      "this is a\n",
      "is a sentence\n",
      "a sentence that\n",
      "sentence that we\n",
      "that we want\n",
      "we want to\n",
      "want to parse\n",
      "to parse and\n",
      "parse and this\n",
      "and this is\n",
      "this is done\n",
      "is done with\n",
      "done with nltk\n",
      "with nltk and\n",
      "nltk and python\n",
      "and python collections\n",
      "python collections None\n",
      "collections None None\n"
     ]
    }
   ],
   "source": [
    "# Produce trigrams from that sentence\n",
    "# (imagine that we are sliding a window of size 3 across the sentence)\n",
    "\n",
    "for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "    print(w1, w2, w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93303925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep counts\n",
    "\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Generate the trigrams and increase the counter for each occurrence \n",
    "# of a specific trigram\n",
    "for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "    model[(w1, w2)][w3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a16e6310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that appear after \"this is\":\n",
      "word: a\n",
      "word: done\n"
     ]
    }
   ],
   "source": [
    "# Let's see the words that follow \"this is\"\n",
    "print('Words that appear after \"this is\":')\n",
    "for w in model[('this', 'is')]:\n",
    "    print(f'word: {w}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d13c3a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How many times does \"this is\" occur in the sample sentence?\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many times \"this is\" occurs\n",
    "print('\\nHow many times does \"this is\" occur in the sample sentence?')\n",
    "print(sum(model[('this', 'is')].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34df192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How many times does \"a\" occur after \"this is\"?\n",
      "1\n",
      "\n",
      "Words that follow the bigram \"this is\" and the corresponding counts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'done': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how many times word \"a\" occurs after \"this is\"\n",
    "print('\\nHow many times does \"a\" occur after \"this is\"?')\n",
    "print(model[('this', 'is')]['a'])\n",
    "\n",
    "# What are the words that follow \"this is\" and what are\n",
    "# the corresponding frequencies?\n",
    "print('\\nWords that follow the bigram \"this is\" and the corresponding counts:')\n",
    "dict(model[('this', 'is')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f1958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f2c29de",
   "metadata": {},
   "source": [
    "# Create a simple language model using a text collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "824ddd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put everything together and use a corpus from project Gutenberg\n",
    "# which is provided directly by NLTK\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Create a placeholder for model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count frequency of co-occurance  \n",
    "for sentence in gutenberg.sents('shakespeare-macbeth.txt'):\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    "        \n",
    "# Let's transform the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "312b55ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']']\n",
      "['Actus', 'Primus', '.']\n",
      "['Scoena', 'Prima', '.']\n",
      "['Thunder', 'and', 'Lightning', '.']\n",
      "['Enter', 'three', 'Witches', '.']\n",
      "['1', '.']\n",
      "['When', 'shall', 'we', 'three', 'meet', 'againe', '?']\n",
      "['In', 'Thunder', ',', 'Lightning', ',', 'or', 'in', 'Raine', '?']\n",
      "['2', '.']\n",
      "['When', 'the', 'Hurley', '-', 'burley', \"'\", 's', 'done', ',', 'When', 'the', 'Battaile', \"'\", 's', 'lost', ',', 'and', 'wonne']\n",
      "['3', '.']\n",
      "['That', 'will', 'be', 'ere', 'the', 'set', 'of', 'Sunne']\n",
      "['1', '.']\n",
      "['Where', 'the', 'place', '?']\n",
      "['2', '.']\n",
      "['Vpon', 'the', 'Heath']\n",
      "['3', '.']\n",
      "['There', 'to', 'meet', 'with', 'Macbeth']\n",
      "['1', '.']\n",
      "['I', 'come', ',', 'Gray', '-', 'Malkin']\n",
      "['All', '.']\n",
      "['Padock', 'calls', 'anon', ':', 'faire', 'is', 'foule', ',', 'and', 'foule', 'is', 'faire', ',', 'Houer', 'through', 'the', 'fogge', 'and', 'filthie', 'ayre', '.']\n",
      "['Exeunt', '.']\n",
      "['Scena', 'Secunda', '.']\n",
      "['Alarum', 'within', '.']\n",
      "['Enter', 'King', 'Malcome', ',', 'Donalbaine', ',', 'Lenox', ',', 'with', 'attendants', ',', 'meeting', 'a', 'bleeding', 'Captaine', '.']\n",
      "['King', '.']\n",
      "['What', 'bloody', 'man', 'is', 'that', '?']\n",
      "['he', 'can', 'report', ',', 'As', 'seemeth', 'by', 'his', 'plight', ',', 'of', 'the', 'Reuolt', 'The', 'newest', 'state']\n",
      "['Mal', '.']\n",
      "['This', 'is', 'the', 'Serieant', ',', 'Who', 'like', 'a', 'good', 'and', 'hardie', 'Souldier', 'fought', \"'\", 'Gainst', 'my', 'Captiuitie', ':', 'Haile', 'braue', 'friend', ';', 'Say', 'to', 'the', 'King', ',', 'the', 'knowledge', 'of', 'the', 'Broyle', ',', 'As', 'thou', 'didst', 'leaue', 'it']\n",
      "['Cap', '.']\n",
      "['Doubtfull', 'it', 'stood', ',', 'As', 'two', 'spent', 'Swimmers', ',', 'that', 'doe', 'cling', 'together', ',', 'And', 'choake', 'their', 'Art', ':', 'The', 'mercilesse', 'Macdonwald', '(', 'Worthie', 'to', 'be', 'a', 'Rebell', ',', 'for', 'to', 'that', 'The', 'multiplying', 'Villanies', 'of', 'Nature', 'Doe', 'swarme', 'vpon', 'him', ')', 'from', 'the', 'Westerne', 'Isles', 'Of', 'Kernes', 'and', 'Gallowgrosses', 'is', 'supply', \"'\", 'd', ',', 'And', 'Fortune', 'on', 'his', 'damned', 'Quarry', 'smiling', ',', 'Shew', \"'\", 'd', 'like', 'a', 'Rebells', 'Whore', ':', 'but', 'all', \"'\", 's', 'too', 'weake', ':', 'For', 'braue', 'Macbeth', '(', 'well', 'hee', 'deserues', 'that', 'Name', ')', 'Disdayning', 'Fortune', ',', 'with', 'his', 'brandisht', 'Steele', ',', 'Which', 'smoak', \"'\", 'd', 'with', 'bloody', 'execution', '(', 'Like', 'Valours', 'Minion', ')', 'caru', \"'\", 'd', 'out', 'his', 'passage', ',', 'Till', 'hee', 'fac', \"'\", 'd', 'the', 'Slaue', ':', 'Which', 'neu', \"'\", 'r', 'shooke', 'hands', ',', 'nor', 'bad', 'farwell', 'to', 'him', ',', 'Till', 'he', 'vnseam', \"'\", 'd', 'him', 'from', 'the', 'Naue', 'toth', \"'\", 'Chops', ',', 'And', 'fix', \"'\", 'd', 'his', 'Head', 'vpon', 'our', 'Battlements']\n",
      "['King', '.']\n",
      "['O', 'valiant', 'Cousin', ',', 'worthy', 'Gentleman']\n",
      "['Cap', '.']\n",
      "['As', 'whence', 'the', 'Sunne', \"'\", 'gins', 'his', 'reflection', ',', 'Shipwracking', 'Stormes', ',', 'and', 'direfull', 'Thunders', ':', 'So', 'from', 'that', 'Spring', ',', 'whence', 'comfort', 'seem', \"'\", 'd', 'to', 'come', ',', 'Discomfort', 'swells', ':', 'Marke', 'King', 'of', 'Scotland', ',', 'marke', ',', 'No', 'sooner', 'Iustice', 'had', ',', 'with', 'Valour', 'arm', \"'\", 'd', ',', 'Compell', \"'\", 'd', 'these', 'skipping', 'Kernes', 'to', 'trust', 'their', 'heeles', ',', 'But', 'the', 'Norweyan', 'Lord', ',', 'surueying', 'vantage', ',', 'With', 'furbusht', 'Armes', ',', 'and', 'new', 'supplyes', 'of', 'men', ',', 'Began', 'a', 'fresh', 'assault']\n",
      "['King', '.']\n",
      "['Dismay', \"'\", 'd', 'not', 'this', 'our', 'Captaines', ',', 'Macbeth', 'and', 'Banquoh', '?']\n",
      "['Cap', '.']\n",
      "['Yes', ',', 'as', 'Sparrowes', ',', 'Eagles', ';', 'Or', 'the', 'Hare', ',', 'the', 'Lyon', ':', 'If', 'I', 'say', 'sooth', ',', 'I', 'must', 'report', 'they', 'were', 'As', 'Cannons', 'ouer', '-', 'charg', \"'\", 'd', 'with', 'double', 'Cracks', ',', 'So', 'they', 'doubly', 'redoubled', 'stroakes', 'vpon', 'the', 'Foe', ':', 'Except', 'they', 'meant', 'to', 'bathe', 'in', 'reeking', 'Wounds', ',', 'Or', 'memorize', 'another', 'Golgotha', ',', 'I', 'cannot', 'tell', ':', 'but', 'I', 'am', 'faint', ',', 'My', 'Gashes', 'cry', 'for', 'helpe']\n",
      "['King', '.']\n",
      "['So', 'well', 'thy', 'words', 'become', 'thee', ',', 'as', 'thy', 'wounds', ',', 'They', 'smack', 'of', 'Honor', 'both', ':', 'Goe', 'get', 'him', 'Surgeons', '.']\n",
      "['Enter', 'Rosse', 'and', 'Angus', '.']\n",
      "['Who', 'comes', 'here', '?']\n",
      "['Mal', '.']\n",
      "['The', 'worthy', 'Thane', 'of', 'Rosse']\n",
      "['Lenox', '.']\n",
      "['What', 'a', 'haste', 'lookes', 'through', 'his', 'eyes', '?']\n",
      "['So', 'should', 'he', 'looke', ',', 'that', 'seemes', 'to', 'speake', 'things', 'strange']\n"
     ]
    }
   ],
   "source": [
    "# Print a sample of the training sentences\n",
    "for i in range(0, 50):\n",
    "    print(gutenberg.sents('shakespeare-macbeth.txt')[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e5fc52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faint': 0.034482758620689655,\n",
       " 'Thane': 0.06896551724137931,\n",
       " 'fed': 0.034482758620689655,\n",
       " 'his': 0.034482758620689655,\n",
       " 'settled': 0.034482758620689655,\n",
       " 'afraid': 0.06896551724137931,\n",
       " 'one': 0.034482758620689655,\n",
       " 'recklesse': 0.034482758620689655,\n",
       " 'cabin': 0.034482758620689655,\n",
       " 'a': 0.034482758620689655,\n",
       " 'bent': 0.034482758620689655,\n",
       " 'in': 0.06896551724137931,\n",
       " 'for': 0.034482758620689655,\n",
       " 'call': 0.034482758620689655,\n",
       " 'so': 0.034482758620689655,\n",
       " 'not': 0.06896551724137931,\n",
       " 'perfect': 0.034482758620689655,\n",
       " 'too': 0.034482758620689655,\n",
       " 'yong': 0.034482758620689655,\n",
       " 'as': 0.034482758620689655,\n",
       " 'yet': 0.034482758620689655,\n",
       " 'truly': 0.034482758620689655,\n",
       " ',': 0.034482758620689655,\n",
       " 'sure': 0.034482758620689655,\n",
       " 'sick': 0.034482758620689655}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we have trained the model.\n",
    "# Let's see the probabilities of words that may follow the sequence \"I am\"\n",
    "\n",
    "dict(model['I', 'am'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "547ae8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am as I am yet Vnknowne to Woman , neuer was forsworne , Scarsely haue coueted what was mine owne .\n"
     ]
    }
   ],
   "source": [
    "# Let's use the model to generate some text!\n",
    "\n",
    "import random\n",
    "\n",
    "# We will tell the model to start with the words \"I am\"\n",
    "# So, it will produce the next word.\n",
    "# Then, the process is repeated using the generated word and the previous one.\n",
    "\n",
    "text = [\"I\", \"am\"]\n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "    probs = []\n",
    "    population = []\n",
    "    for word in model[tuple(text[-2:])].keys():\n",
    "        probs.append(model[tuple(text[-2:])][word])\n",
    "        population.append(word)\n",
    "    token = random.choices(population, probs, k=1)\n",
    "    text.append(token[0])    \n",
    "\n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "\n",
    "print(' '.join([t for t in text if t]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51327360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f5b2f78",
   "metadata": {},
   "source": [
    "# Use directly the MLE from nltk python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44b2ed8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['thunder', 'and', 'lightning'],\n",
       " ['enter', 'three', 'witches'],\n",
       " ['i', 'am', 'faint'],\n",
       " ['god', 'saue', 'the', 'king'],\n",
       " ['looke', 'what', 'i', 'haue', 'here'],\n",
       " ['here', 'the', 'lies', 'haue', 'the', 'eyes']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this simple example we will calculate perplexities of test sentences\n",
    "# for a simple model that is trained in a tiny training set\n",
    "\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Vocabulary\n",
    "\n",
    "train_sentences = ['Thunder and Lightning',\n",
    "                   'Enter three Witches',\n",
    "                   'I am faint',\n",
    "                   'God saue the King',\n",
    "                   'Looke what I haue here',\n",
    "                   'Here the lies haue the eyes'\n",
    "                  ]\n",
    "\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in train_sentences]\n",
    "\n",
    "# Print the tokenized training set\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23bbac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2 # Highest n-gram order for the Maximul Likelihood Estimator\n",
    "\n",
    "# Prepare training data:\n",
    "# Use bigrams, and mark the start and end of the sentence\n",
    "train_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "words = [word for sent in tokenized_text for word in sent]\n",
    "words.extend([\"<s>\", \"</s>\"])\n",
    "padded_vocab = Vocabulary(words)\n",
    "\n",
    "# Fit model\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d51f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE estimates for test data:\n",
      "\n",
      "MLE Estimates for sentence 0: [(('thunder', ('<s>',)), 0.16666666666666666), (('and', ('thunder',)), 1.0), (('lightning', ('and',)), 1.0), (('</s>', ('lightning',)), 1.0)]\n",
      "\n",
      "MLE Estimates for sentence 1: [(('through', ('<s>',)), 0.0), (('his', ('through',)), 0), (('eyes', ('his',)), 0), (('</s>', ('eyes',)), 1.0)]\n",
      "\n",
      "MLE Estimates for sentence 2: [(('i', ('<s>',)), 0.16666666666666666), (('haue', ('i',)), 0.5), (('the', ('haue',)), 0.5), (('king', ('the',)), 0.3333333333333333), (('</s>', ('king',)), 1.0)]\n",
      "\n",
      "Perplexities:\n",
      "PP(Thunder and lightning):1.5650845800732873\n",
      "PP(through his eyes):inf\n",
      "PP(I haue the king):2.352158045049347\n"
     ]
    }
   ],
   "source": [
    "# We have trained the bigram model.\n",
    "# Let's test it in some test sentences\n",
    "\n",
    "# We will on purpose include a sentence that appears 'as is' in the tranining set.\n",
    "# That sentence should have the highest perplexity\n",
    "# The other two sentences do not appear in the training set:\n",
    "# The first one contains bigrams that the model has never seen before,\n",
    "# but the last sentence contains bigrams that the model has learned\n",
    "test_sentences = [\n",
    "    'Thunder and lightning',   # So, this should have the lowest perplexity (the model explains well the sentence)\n",
    "    'through his eyes',        # This one should have PP that equals infinity (due to zero probabilities)\n",
    "    'I haue the king']         # This sentence can be explained but it will surprise the model more than the 1st\n",
    "\n",
    "\n",
    "# Tokenize the test sentences\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in test_sentences]\n",
    "\n",
    "# For each test sentence print the MLE estimates for the bigrams that need to be calculated\n",
    "print('MLE estimates for test data:')\n",
    "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "for i,test in enumerate(test_data):\n",
    "    print (f'\\nMLE Estimates for sentence {i}:', [((ngram[-1], ngram[:-1]),model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
    "\n",
    "# For each test sentence print the perplexities of the model\n",
    "print('\\nPerplexities:')\n",
    "# Reset the test_data, since the generator has been exhausted\n",
    "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "for i, test in enumerate(test_data):\n",
    "    print(\"PP({0}):{1}\".format(test_sentences[i], model.perplexity(test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da5b0371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE estimates for test data:\n",
      "\n",
      "MLE Estimates for sentence 0: [(('thunder', ('<s>',)), 0.06896551724137931), (('and', ('thunder',)), 0.08333333333333333), (('lightning', ('and',)), 0.08333333333333333), (('</s>', ('lightning',)), 0.08333333333333333)]\n",
      "\n",
      "MLE Estimates for sentence 1: [(('through', ('<s>',)), 0.034482758620689655), (('his', ('through',)), 0.043478260869565216), (('eyes', ('his',)), 0.043478260869565216), (('</s>', ('eyes',)), 0.08333333333333333)]\n",
      "\n",
      "MLE Estimates for sentence 2: [(('i', ('<s>',)), 0.06896551724137931), (('haue', ('i',)), 0.08), (('the', ('haue',)), 0.08), (('king', ('the',)), 0.07692307692307693), (('</s>', ('king',)), 0.08333333333333333)]\n",
      "\n",
      "Perplexities:\n",
      "PP(Thunder and lightning):12.581370016785733\n",
      "PP(through his eyes):20.713749936746982\n",
      "PP(I haue the king):12.87248887971409\n"
     ]
    }
   ],
   "source": [
    "# Same example with Laplace smoothing\n",
    "\n",
    "# This is the exact same code as above.\n",
    "# The only difference is that we use Laplace() instead of MLE() to define the model\n",
    "\n",
    "from nltk.lm import Laplace\n",
    "\n",
    "train_sentences = ['Thunder and Lightning',\n",
    "                   'Enter three Witches',\n",
    "                   'I am faint',\n",
    "                   'God saue the King',\n",
    "                   'Looke what I haue here',\n",
    "                   'Here the lies haue the eyes'\n",
    "                  ]\n",
    "\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in train_sentences]\n",
    "\n",
    "n = 2 \n",
    "\n",
    "train_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "words = [word for sent in tokenized_text for word in sent]\n",
    "words.extend([\"<s>\", \"</s>\"])\n",
    "padded_vocab = Vocabulary(words)\n",
    "\n",
    "# Fit model\n",
    "model = Laplace(n)\n",
    "model.fit(train_data, padded_vocab)\n",
    "\n",
    "test_sentences = [\n",
    "    'Thunder and lightning',   \n",
    "    'through his eyes',        \n",
    "    'I haue the king']         \n",
    "\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in test_sentences]\n",
    "\n",
    "print('MLE estimates for test data:')\n",
    "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "for i,test in enumerate(test_data):\n",
    "    print (f'\\nMLE Estimates for sentence {i}:', [((ngram[-1], ngram[:-1]),model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
    "\n",
    "print('\\nPerplexities:')\n",
    "# Reset the test_data, since the generator has been exhausted\n",
    "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "for i, test in enumerate(test_data):\n",
    "    print(\"PP({0}):{1}\".format(test_sentences[i], model.perplexity(test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d74f1ad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad96f4",
   "metadata": {},
   "source": [
    "### In the last example we do not have zero probabilities or perplexities that go to infinity, because we have performed smoothing. We have stolen some of the probability mass of other n-grams to slightly augment the zero probabilities of unseen n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c513f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
